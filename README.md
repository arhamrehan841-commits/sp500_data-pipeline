ğŸŒ S&P 500 Intraday Data Pipeline ğŸš€

An automated ETL pipeline that extracts, transforms, and loads real-time intraday data for the S&P 500 using Apache Airflow running in Docker.

The pipeline fetches 1-minute interval stock data from Yahoo Finance, processes it for advanced analytics, and seamlessly loads it into Amazon S3 and Snowflake for storage and visualization. ğŸ“ˆâ„ï¸

ğŸ“‹ Table of Contents

ğŸ“– Project Overview

âš™ï¸ Technologies Used

ğŸ“ Prerequisites

ğŸ”§ Setup

ğŸ³ Docker Setup

ğŸ› ï¸ Airflow Setup

ğŸš€ Usage

ğŸ’¡ Contributing

ğŸ“œ License

ğŸ“– Project Overview

This project leverages Apache Airflow for orchestration and scheduling of an ETL pipeline that processes real-time financial data.

Workflow Summary:

Extract â†’ Pulls 1-minute interval data for the top 10 S&P 500 tickers from Yahoo Finance using yfinance.

Transform â†’ Enhances the dataset with calculated minute returns, trading hours, and additional metrics for analysis.

Load â†’ Pushes the cleaned, transformed data to both Amazon S3 and Snowflake for long-term storage and querying.

All tasks are automated and executed within Dockerized Airflow containers, ensuring a reproducible and isolated environment. ğŸ§©

âš™ï¸ Technologies Used
Tool	Purpose
ğŸ³ Apache Airflow	Workflow orchestration and scheduling
ğŸ‹ Docker / Docker Compose	Containerization and environment management
ğŸ“Š yfinance	Fetching real-time market data
ğŸ§¹ pandas	Data cleaning and transformation
â˜ï¸ Amazon S3	Cloud data storage
â„ï¸ Snowflake	Data warehousing and analytics
ğŸ Python 3.x	Primary programming language
ğŸ“ Prerequisites

Before setup, ensure you have:

âœ… Docker & Docker Compose â†’ Install Docker

âœ… Python 3.x installed
âœ… AWS Account â†’ S3 credentials for storage
âœ… Snowflake Account â†’ Database and warehouse access
âœ… Git for cloning the repository

ğŸ”§ Setup
ğŸ³ Docker Setup

1ï¸âƒ£ Clone the Repository

git clone <repository_url>
cd <repository_directory>


2ï¸âƒ£ Build and Start Containers

sudo docker-compose up --build -d


3ï¸âƒ£ Access Airflow UI
Open: ğŸ‘‰ http://localhost:8080

Login Credentials:

Username: airflow
Password: airflow

ğŸ› ï¸ Airflow Setup

1ï¸âƒ£ Initialize Airflow Database

sudo docker-compose run --rm airflow-init


2ï¸âƒ£ Create Connections

AWS Connection â†’ Add your AWS Access Key & Secret Key in the Airflow UI

Snowflake Connection â†’ Add your Snowflake credentials (user, password, account, schema)

3ï¸âƒ£ Schedule the DAG
Airflow will automatically schedule and execute the ETL DAG as per its defined frequency (@daily, or as configured).

ğŸš€ Usage

Once deployed, your Airflow DAG (sp500_intraday_dag.py) automates the full ETL process.

ğŸ” Automation

The pipeline runs automatically based on your chosen schedule.

Each step â€” Extract, Transform, Load â€” is fully monitored via the Airflow UI.

ğŸ“Š Monitoring

In the Airflow UI, you can:

âœ… Track task success/failure

ğŸ“œ View detailed logs

ğŸ” Trigger manual runs as needed

ğŸ“¦ Data Outputs

Amazon S3:
Processed data stored under

s3://your-bucket/sp500_intraday/{trading_date}.csv


Snowflake:
Data loaded into Snowflake tables for advanced analysis and dashboarding.

ğŸ’¡ Contributing

We welcome all contributions! ğŸ§ 

Fork the repository

Create a feature branch

git checkout -b feature-name


Commit your changes

git commit -am "Add new feature"


Push your branch

git push origin feature-name


Open a Pull Request to merge your changes into main

ğŸ“œ License

Licensed under the MIT License.
See the LICENSE
 file for full details.

ğŸ’¬ Made with ğŸ’» + ğŸ“ˆ to automate the future of financial data.
