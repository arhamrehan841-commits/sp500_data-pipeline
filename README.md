# ğŸŒ S&P 500 Intraday Data Pipeline 

An automated ETL pipeline that extracts, transforms, and loads real-time intraday data for the S&P 500 using Apache Airflow running in Docker.

The pipeline fetches 1-minute interval stock data from Yahoo Finance, processes it for analytics, and loads it into Amazon S3 and Snowflake for storage and visualization. ğŸ“ˆâ„ï¸

## ğŸ“‹ Table of Contents

### ğŸ“– [Project Overview](#ğŸ“–-Project-Overview)

### âš™ï¸ [Technologies Used](#Technologies-Used)

### ğŸ“ [Prerequisites](#Prerequisites)

### ğŸ”§ [Setup](#Setup)

ğŸ³ [Docker Setup](#Docker-Setup)

ğŸ› ï¸ [Airflow Setup](#Airflow-Setup)

### ğŸš€ [Usage](#Usage)

ğŸ” [Automation](#Automation)

ğŸ“Š [Monitoring](#Monitoring)

### ğŸ“¦ [Data Outputs](#Data-Outputs)

### ğŸ’¡ [Contributing](#Contributing)

### ğŸ“œ [License](#License)

## ğŸ“– Project Overview

This project uses Apache Airflow to orchestrate an ETL pipeline for real-time S&P 500 data.

Workflow Summary:

Extract: Pulls 1-minute interval data for the top 10 S&P 500 tickers using yfinance.

Transform: Adds minute returns, trading hours, and other metrics.

Load: Uploads cleaned data to Amazon S3 and Snowflake for long-term storage and analysis.

All tasks run in Dockerized Airflow containers, ensuring a reproducible and isolated environment. ğŸ§©

## âš™ï¸ Technologies Used
Tool	Purpose
ğŸ³ Apache Airflow	Workflow orchestration and scheduling
ğŸ‹ Docker / Docker Compose	Containerization and environment management
ğŸ“Š yfinance	Fetching real-time market data
ğŸ§¹ pandas	Data cleaning and transformation
â˜ï¸ Amazon S3	Cloud data storage
â„ï¸ Snowflake	Data warehousing and analytics
ğŸ Python 3.x	Primary programming language
ğŸ“ Prerequisites

Before setup, make sure you have:

âœ… Docker & Docker Compose â€” Install Docker

âœ… Python 3.x installed

âœ… AWS Account â€” S3 credentials for storage

âœ… Snowflake Account â€” Database and warehouse access

âœ… Git for cloning the repository

## ğŸ”§ Setup
### ğŸ³ Docker Setup

Clone the repository:

git clone <repository_url>
cd <repository_directory>


Build and start services:

sudo docker-compose up --build -d


Access Airflow UI: http://localhost:8080

Login Credentials:

Username: airflow
Password: airflow

### ğŸ› ï¸ Airflow Setup

Initialize Airflow database:

sudo docker-compose run --rm airflow-init


Configure connections in Airflow UI:

AWS S3 Connection: Conn ID aws_default â€” Access Key & Secret Key

Snowflake Connection: Conn ID snowflake_default â€” Account, User, Password, Database, Schema, Warehouse

Schedule the ETL DAG
Runs automatically based on your configured schedule (@daily or custom).

## ğŸš€ Usage
### ğŸ” Automation

Pipeline executes automatically based on your schedule.

All tasks (Extract â†’ Transform â†’ Load) are fully traceable in Airflow.

### ğŸ“Š Monitoring

Use the Airflow UI (http://localhost:8080
) to:

Track task success/failure âœ…âŒ

View real-time logs ğŸ“

Trigger manual DAG runs ğŸ”„

## ğŸ“¦ Data Outputs

### Amazon S3:

s3://your-bucket/sp500_intraday/{trading_date}.csv


Partitioned by trading date for efficient querying.

### Snowflake:

Data loaded into:

DATABASE.SP500_SCHEMA.INTRADAY_DATA


Ready for BI tools (Tableau, Power BI) and SQL analytics.

## ğŸ’¡ Contributing

### We welcome contributions!

Fork the repository ğŸ´

Create a feature branch:

git checkout -b feature/your-feature-name


### Commit your changes:

git commit -m "Add: your feature description"


Push and open a Pull Request:

git push origin feature/your-feature-name

## ğŸ“œ License

Licensed under the MIT License. See the [LICENSE](LICENSE.txt)
 file for full details.

### ğŸ’¬ Made with ğŸ’» + ğŸ“ˆ to automate the future of financial analytics.
